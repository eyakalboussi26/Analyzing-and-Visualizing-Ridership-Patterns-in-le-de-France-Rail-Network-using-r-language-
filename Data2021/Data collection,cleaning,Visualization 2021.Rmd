---
title: 'Data collection,cleaning,Visualization 2021'
author: "eya kalboussi"
date: "2023-12-20"
output: html_document
---

### 1. Import Data

```{r}
##  Load the 2021 data
NbValidS1 <- read.delim("/cloud/project/Data2021/NbValidS1.txt")
NbValidS2 <- read.delim("/cloud/project/Data2021/NbValidS2.txt")
ProfilFerS1 <- read.delim("/cloud/project/Data2021/ProfilFerS1.txt", header=FALSE)
ProfilFerS2 <- read.delim2("/cloud/project/Data2021/ProfilFerS2.txt", header=FALSE)
```

### 2. Data Exploration: understanding the data, exploring its distribution, and identifying potential

#### A. "NbValidS1" Data set:

```{r}
head(NbValidS1)
```

```{r}
str(NbValidS1)
```

```{r}
summary(NbValidS1)
```

```{r}
dim(NbValidS1)

```

```{r}
names(NbValidS1)
```

```{r}
nrow(NbValidS1)
```

```{r}
length(NbValidS1)
```

```         
```

### 3. Data cleaning : The data cleaning process involves, first and foremost, handling missing values, outliers, and any inconsistencies

```{r}
# Display rows with missing values before deletion
rows_with_missing_before <- which(rowSums(is.na(NbValidS1) | NbValidS1 == "inconnu" | NbValidS1 == "?" | NbValidS1 == "NA" | NbValidS1 == "ND" | NbValidS1 == "0" | NbValidS1 == "") > 0)
cat("Number of rows with missing values before deletion:", length(rows_with_missing_before), "\n")
cat("Rows with missing values before deletion:\n")
print(NbValidS1[rows_with_missing_before, ])

```

```{r}
# Load the 'dplyr' library
library(dplyr)

# Remove rows containing specific values across the entire dataset
NbValidS1_cleaned <- NbValidS1 %>%
  filter_all(all_vars(!(. %in% c("inconnu", "?", "NA", "ND", "0", "", "NOT DEFINED"))))

# Display the new dataset without specific values
print(NbValidS1_cleaned)

# Remove rows with NA values in the 'NB_VALD' column
NbValidS1_cleaned <- NbValidS1_cleaned[!is.na(NbValidS1_cleaned$NB_VALD), ]


```

```{r}
#display the dimension of the new datset
dim(NbValidS1_cleaned)
```

Discussion:

Initially, the dataset **"NbValidS1"**dimension is 1064019. We detected that there are 117142 rows containing missing values. We removed them and rechecked the data dimension, which is now 946877.

### **4. Data analysis and Visualization**

A. Temporal Trends : Distribution of Validations per Day (Jour):

```{r}
library(dplyr)
library(ggplot2)

# Extract a subset of NbValidS1_cleaned for the row with the maximum value, including the columns 'JOUR', 'NB_VALD', and 'LIBELLE_ARRET'
result <- NbValidS1_cleaned[which.max(NbValidS1_cleaned$NB_VALD), c("JOUR", "NB_VALD")]

# Display the result
print(result)

# Create a plot of the number of validations per day
plot_result <- ggplot(result, aes(x = JOUR, y = NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Day", y = "Number of Validations", title = "Day with Maximum number of validations S1 2021")

# Show the graph
print(plot_result)

# Save the graph as an image file (e.g., ".png", ".jpg")
ggsave("Day with Maximum number of validations S1 2021.png", plot_result, width = 10, height = 6, units = "in")


```

##### B. Spatial distribution : Distribution of Validations per Transport Stop label, type of ticket, (Time Slot, Stop Label, and Ticket Category)

```{r}
# Extract a subset of NbValidS1_cleaned for the row with the maximum value,including the columns 'JOUR', 'NB_VALD', and 'LIBELLE_ARRET'
result <- NbValidS1_cleaned[which.max(NbValidS1_cleaned$NB_VALD), c( "JOUR","LIBELLE_ARRET", "NB_VALD","CATEGORIE_TITRE")]
#display the resultat
print(result)


```

```{r}
# Replace "NbValidS1_cleaned" with the actual name of your dataset
lowiest_vald_S1 <- NbValidS1_cleaned %>%
  arrange(NB_VALD) %>%
  slice_head(n = 2)

# Display the two rows representing the categories with the lowiest validation values
print(lowiest_vald_S1)

# Save the two rows to a CSV file
write.csv(lowiest_vald_S1, "lowiest_vald_S1.cvs", row.names = FALSE)


```

```{r}
# Convert the column NB_VALD to a numeric type
NbValidS1_cleaned$NB_VALD <- as.numeric(as.character(NbValidS1_cleaned$NB_VALD))

# Create a graph showing the distribution of ticket categories based on the number of validations
transport_ticket_validations_dist_S1_2023 <- ggplot(data = NbValidS1_cleaned, aes(x = CATEGORIE_TITRE, y = NB_VALD)) +
  geom_bar(stat = "summary", fun = "sum", aes(fill = CATEGORIE_TITRE)) +
  labs(x = "Ticket Category", y = "Total Validations", title = "Distribution of Ticket Types by Validations 2023") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Save the graph as an image file (e.g., ".png", ".jpg")
ggsave("transport_ticket_validations_dist_S1_2023.png", transport_ticket_validations_dist_S1_2023, width = 10, height = 6, units = "in")

# Display the graph
print(transport_ticket_validations_dist_S1_2023)

```

```         
```

### The impact of the coronavirus on the number of validations

-   During the first semester of 2021, starting in January and ending in June, our graphical analyses revealed that "Navigo" had the highest number of validations among all transportation passes 73730 .

-   We can conclude that during this period, "Navigo" was the best-selling ticket. Additionally, the stations recording the highest number of validations were "SAINT-LAZARE".

-   Following Navigo, the transportation passes "IMAGINE R" and "TST"were also notable.Also We found that the stations" PORTE MAILLOT" and" BOULOGNE-PONT DE SAINT" have the fewest validations, specifically fewer than 5 With transport ticket title is "NAVIGO JOUR" in 01/01/2021.

-   The"30/06/2021 " is the day that had the highest number of validations, totaling 73730

-   Comparing 2021 and 2019, we see that the number of validations for transport tickets of type NAVIGO increased from 127535 to 73730.

-   So, the regression rate os rate or percentage increase between 2021 and 2019 for the number of validations of NAVIGO transport tickets is approximately 24.17%

=\> The impact of the coronavirus on the number of validations explains the decrease in the number of validations due to the lockdown the period of confinement from March 20th to May 3rd, 2021 and the emergence of telecommuting during the years 2017 to 2019. During that time, La Défense - Grande Arche was the station with the most validations in Île-de-France. However, in 2021, Saint Lazare has the highest number of validations.

#### 4. Number of ticket validations during the period of confinement from March 20th to May 3rd, 2021

```{r}
# Load necessary libraries
library(dplyr)

# Convert the JOUR column to date format if it's not already in that format
NbValidS1_cleaned$JOUR <- as.Date(NbValidS1_cleaned$JOUR, format = "%d/%m/%Y")

# Define the confinement and return to normal life periods
normal_life_start <- as.Date("2023-01-01")
normal_life_end <- as.Date("2023-02-18")
confinement_start <- as.Date("2021-03-20")
confinement_end <- as.Date("2021-05-03")


# Filter data for confinement and return to normal life periods
confinement_data <- filter(NbValidS1_cleaned, JOUR >= confinement_start & JOUR <= confinement_end)
normal_life_data <- filter(NbValidS1_cleaned, JOUR >= normal_life_start & JOUR <= normal_life_end)

# Calculate the number of validations for each period
total_validations_confinement <- sum(confinement_data$NB_VALD)
total_validations_normal_life <- sum(normal_life_data$NB_VALD)

# Display the results
print(total_validations_confinement)
print(total_validations_normal_life)


```

From March 20th to May 3rd, 2021: It was the period of a strict lockdown implemented in France due to the COVID-19 pandemic. During this period, there were no validation tickets available.

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

### B. Data cleaning, analysis , Visualization

We will proceed with the other datasets in the same way, following the same steps, to thoroughly analyze the number of validations in S2

#### **B. "NbValidS2" Data set:**

```{r}
NbValidS2 <- read.delim("/cloud/project/Data2021/NbValidS2.txt")
dim(NbValidS2)
```

```{r}
# Search for missing values in all columns
rows_with_missing_before <- which(rowSums(is.na(NbValidS2) | NbValidS2 == "inconnu" | NbValidS2 == "?" | NbValidS2 == "NA" | NbValidS2 == "NON DEFINI" | NbValidS2 == "ND" | NbValidS2 == "0" | NbValidS2 == "") > 0)

# Display the number of rows with missing values before deletion
cat("Number of rows with missing values before deletion:", length(rows_with_missing_before), "\n")

# Display the rows with missing values before deletion
cat("Rows with missing values before deletion:\n")
print(NbValidS2[rows_with_missing_before, ])


# Load the 'dplyr' library
library(dplyr)

# Remove rows containing specific values across the entire dataset
NbValidS2_cleaned <- NbValidS2 %>%
  filter_all(all_vars(!(. %in% c("inconnu", "?", "NA", "ND", "0", "", "NOT DEFINED"))))

# Display the new dataset without specific values
print(NbValidS2_cleaned)

# Remove rows with NA values in the 'NB_VALD' column
NbValidS2_cleaned <- NbValidS2_cleaned[!is.na(NbValidS2_cleaned$NB_VALD), ]


```

Discussion:

Initially, the dataset **"NbValidS2"**dimension is 1084281 . We detected that there are 246654 rows containing missing values. We removed them and rechecked the data dimension, which is now 964705.

#### 1. Temporal Trends : Distribution of Validations per Day (Jour):

```{r}
library(dplyr)
library(ggplot2)

# Extract a subset of NbValidS1_cleaned for the row with the maximum value, including the columns 'JOUR', 'NB_VALD', and 'LIBELLE_ARRET'
result <- NbValidS2_cleaned[which.max(NbValidS2_cleaned$NB_VALD), c("JOUR", "NB_VALD")]

# Display the result
print(result)

# Create a plot of the number of validations per day
plot_result <- ggplot(result, aes(x = JOUR, y = NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Day", y = "Number of Validations", title = "Number of Validations per Day with Maximum S2")

# Show the graph
print(plot_result)
```

#### 2. Spatial distribution : Distribution of Validations per Bus Stop and type of ticket (LIBELLE_ARRET et CATEGORIE_TITRE:

```{r}
# Extract a subset of NbValidS1_cleaned for the row with the maximum value,including the columns 'JOUR', 'NB_VALD', and 'LIBELLE_ARRET'
result <- NbValidS2_cleaned[which.max(NbValidS2_cleaned$NB_VALD), c( "JOUR","LIBELLE_ARRET", "NB_VALD","CATEGORIE_TITRE")]
#display the resultat
print(result)

```

```{r}
# Replace "NbValidS1_cleaned" with the actual name of your dataset
lowiest_vald_S2 <- NbValidS2_cleaned %>%
  arrange(NB_VALD) %>%
  slice_head(n = 2)

# Display the two rows representing the categories with the lowiest validation values
print(lowiest_vald_S2)

# Save the two rows to a CSV file
write.csv(lowiest_vald_S2, "lowiest_vald_S2_data.cvs", row.names = FALSE)
```

```{r}
library(ggplot2)
# Creating a graph representing the distribution of ticket categories by number of validations for these two lines
Ticket_Vald_Distrubution_S2_2023 <- ggplot(NbValidS2_cleaned, aes(x = CATEGORIE_TITRE, y = NB_VALD, fill = CATEGORIE_TITRE)) +
  geom_bar(stat = "identity") +
  labs(x = "Category of Ticket", y = "Number of Validations", title = "Distribution of validations for Ticket Transport category S2 2023") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Save the graph with the file extension (e.g., ".png", ".jpg")
ggsave("Ticket_Vald_Distrubution_S2_2023.png", Ticket_Vald_Distrubution_S2_2023, width = 10, height = 6, units = "in")

# Displaying the graph
print(Ticket_Vald_Distrubution_S2_2023)
```

#### 3. Spatiotemporal Analysis of Transport Ticket Validations during the Second Semester of 2021

-   During the second semester of 2021, starting in July and ending in december, our graphical analyses revealed that "NAVIGO" had the highest number of validations among all transportation passes 90901

-   We can conclude that during this period, "NAVIGO" was the best-selling ticket. Additionally, the stations recording the highest number of validations were "SAINT-LAZARE"

-   Following Navigo, the transportation passes Imagine R and TST were also notable.Also We found that the stations "BREGUET-SABIN" and\
    "CAMPO-FORMIO" have the fewest validations, specifically fewer than 5 With transport ticket title is "NAVIGO JOUR" in 01/07/2021

-   The"18/11/2021 " is the day that had the highest number of validations, totaling 90901.

```{r}



```

```{r}






```

#### 5. Comparison of Ticket Validation Counts between Summer Vacation and Normal School Periods  after the  strict lockdown in 2021

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Convert the JOUR column to date format if it's not already in that format
NbValidS2_cleaned$JOUR <- as.Date(NbValidS2_cleaned$JOUR, format = "%d/%m/%Y")

# Define vacation and school periods
summer_vacation_start <- as.Date("2021-07-08")
summer_vacation_end <- as.Date("2021-09-03")
normal_period_start <- as.Date("2021-09-04")
normal_period_end <- as.Date("2021-10-10")

# Filter data for vacation and school periods
summer_data <- filter(NbValidS2_cleaned, JOUR >= summer_vacation_start & JOUR <= summer_vacation_end)
normal_data <- filter(NbValidS2_cleaned, JOUR >= normal_period_start & JOUR <= normal_period_end)

# Count the number of validations for each ticket type during vacation and school periods
summer_counts <- summer_data %>% 
  group_by(CATEGORIE_TITRE) %>% 
  summarise(Validation_Counts = sum(NB_VALD))

normal_counts <- normal_data %>% 
  group_by(CATEGORIE_TITRE) %>% 
  summarise(Validation_Counts = sum(NB_VALD))

print(summer_counts)
print(normal_counts)

# Merge data for plotting
combined_counts <- rbind(
  mutate(summer_counts, Period = "Summer Vacation"),
  mutate(normal_counts, Period = "Normal Period")
)

# Plot validation counts for ticket types during vacation and school periods
ggplot(combined_counts, aes(x = CATEGORIE_TITRE, y = Validation_Counts, fill = Period)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(title = "Validation Counts for Ticket Types",
       x = "Ticket Type",
       y = "Validation Counts") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels for better readability
  scale_fill_manual(values = c("Summer Vacation" = "skyblue", "Normal Period" = "salmon")) +
  guides(fill = guide_legend(title = "Periods", nrow = 2))  # Add legend for fill colors


```

-   **Effect of Summer Vacation on IMAGINE R Transport Ticket Validations in Two Semesters of 2021 after the strict lockdown**

To assess the impact of summer vacation on the validation count of IMAGINE R transport tickets, we compared two periods within the second semester of 2021.

The first period encompassed the summer school break, starting from ("2021-07-08") to ("2021-09-03"). During this time, we found that the validation count for NAVIGO transport tickets was 101089683 , while for IMAGINE R tickets, it was 24381950

The second period represents the regular timeframe from ("2021-07-04") to ("2021-10-10"). Here, the validation count for NAVIGO tickets was 92485334, and for IMAGINE R tickets, it reached 31424058.

=\> The rate of progress in IMAGINE R ticket validations between Period 1 and Period 2 is approximately 28.87 %., we observe a significant impact from the summer school break on the validation count.

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

#### C. "ProfilFerS1" Data set:

```{r}
dim(ProfilFerS1)
```

```{r}
# Load data from file
ProfilFerS1 <- read.delim("/cloud/project/Data2021/ProfilFerS1.txt", header = FALSE)

# Define column names
colnames(ProfilFerS1) <- c("CODE_STIF_TRNS", "CODE_STIF_RES", "CODE_STIF_ARRET", "LIBELLE_ARRET", "ID_REFA_LDA", "CAT_JOUR", "TRNC_HORR_60", "pourc_validations")

# Identify rows with specific or missing values
rows_to_remove <- which(rowSums(
  is.na(ProfilFerS1) | 
  ProfilFerS1 == "inconnu" | 
  ProfilFerS1 == "?" | 
  ProfilFerS1 == "NA" | 
  ProfilFerS1 == "NOT DEFINED" | 
  ProfilFerS1 == "ND" | 
  ProfilFerS1 == "0" | 
  ProfilFerS1 == ""
) > 0)

# Display the number of rows with specific or missing values before removal
cat("Number of rows with specific or missing values before removal:", length(rows_to_remove), "\n")

# Remove rows with specific or missing values
ProfilFerS1_cleaned <- ProfilFerS1[-rows_to_remove, ]

# Display the dimensions of the new dataset after cleaning
cat("Dimensions of the dataset after cleaning:", dim(ProfilFerS1_cleaned), "\n")



```

```         
```

Discussion:

Initially, the dataset **"ProfilFerS1"**dimension is 80752. We detected that there are 211 rows containing missing values. We removed them and rechecked the data dimension, which is now 80531.

#### 6. Determining the High Validation Percentage with Day Category, Locations, and Time Slot:

```{r}
# Commenting the code for English readability
library(dplyr)

# Convert the 'pourc_validations' column to numeric if necessary
ProfilFerS1_cleaned$pourc_validations <- as.numeric(as.character(ProfilFerS1_cleaned$pourc_validations))

# Find the row with the maximum percentage of validations
max_pourc_validations_row <- ProfilFerS1_cleaned %>%
  filter(pourc_validations == max(pourc_validations, na.rm = TRUE))

# Display the row with the maximum percentage of validations
print(max_pourc_validations_row)


```

Discussion:

While analyzing the ProfilFerS1 dataset to determine the type of day with the most validations, we discovered that 'SAVS' corresponds to a Saturday during school holidays, with a validation percentage of 100%. The location label is 'Dourdan la foret,' and the time slot is from 4 PM to 5 PM.

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

#### D. "ProfilFerS2" Data set:

```{r}
# Load data from file
ProfilFerS2 <- read.delim2("/cloud/project/Data2021/ProfilFerS2.txt", header=FALSE)

# Define column names
colnames(ProfilFerS2) <- c("CODE_STIF_TRNS", "CODE_STIF_RES", "CODE_STIF_ARRET", "LIBELLE_ARRET", "ID_REFA_LDA", "CAT_JOUR", "TRNC_HORR_60", "pourc_validations")

# Identify rows with specific or missing values
rows_to_remove <- which(rowSums(
  is.na(ProfilFerS2) | 
  ProfilFerS2 == "Inconnu" | 
  ProfilFerS2 == "?" | 
  ProfilFerS2 == "NA" | 
  ProfilFerS2 == "NOT DEFINED" | 
  ProfilFerS2 == "ND" | 
  ProfilFerS2 == "0" | 
  ProfilFerS2 == ""
) > 0)

# Display the number of rows with specific or missing values before removal
cat("Number of rows with specific or missing values before removal:", length(rows_to_remove), "\n")

# Remove rows with specific or missing values
ProfilFerS2_cleaned <- ProfilFerS1[-rows_to_remove, ]

# Display the dimensions of the new dataset after cleaning
cat("Dimensions of the dataset after cleaning:", dim(ProfilFerS2_cleaned), "\n")
```

Discussion:

Initially, the dataset **"ProfilFerS2"**dimension is 83174. We detected that there are 120 rows containing missing values. We removed them and rechecked the data dimension, which is now 80632.

```{r}
# Commenting the code for English readability
library(dplyr)

# Convert the 'pourc_validations' column to numeric if necessary
ProfilFerS2_cleaned$pourc_validations <- as.numeric(as.character(ProfilFerS2_cleaned$pourc_validations))

# Find the row with the maximum percentage of validations
max_pourc_validations_row_S2 <- ProfilFerS2_cleaned %>%
  filter(pourc_validations == max(pourc_validations, na.rm = TRUE))

# Display the row with the maximum percentage of validations
print(max_pourc_validations_row_S2)

```
