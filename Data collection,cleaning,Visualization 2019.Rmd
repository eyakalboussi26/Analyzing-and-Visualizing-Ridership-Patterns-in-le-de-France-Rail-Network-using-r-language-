---
title: 'Data collection,cleaning,Visualization 2019'
author: "eya kalboussi"
date: "2024-01-02"
output: html_document
---

### 1. Import Data

```{r}
##  Load the 2019 data
NbValidS1 <- read.delim("/cloud/project/Data2019/NbValidS1.txt")
NbValidS2 <- read.delim("/cloud/project/Data2019/NbValidS2.txt")
ProfilFerS1 <- read.delim("/cloud/project/Data2019/ProfilFerS1.txt", header=FALSE)
ProfilFerS2 <- read.delim2("/cloud/project/Data2019/ProfilFerS2.txt", header=FALSE)
```

### 2. Data Exploration: understanding the data, exploring its distribution, and identifying potential

#### A. "NbValidS1" Data set:

```{r}
head(NbValidS1)
```

```{r}
str(NbValidS1)
```

```{r}
summary(NbValidS1)
```

```{r}
dim(NbValidS1)

```

```{r}
names(NbValidS1)
```

```{r}
nrow(NbValidS1)
```

```{r}
length(NbValidS1)
```

```         
```

### 3. Data cleaning : The data cleaning process involves, first and foremost, handling missing values, outliers, and any inconsistencies

```{r}
# Display rows with missing values before deletion
rows_with_missing_before <- which(rowSums(is.na(NbValidS1) | NbValidS1 == "inconnu" | NbValidS1 == "?" | NbValidS1 == "NA" | NbValidS1 == "ND" | NbValidS1 == "0" | NbValidS1 == "") > 0)
cat("Number of rows with missing values before deletion:", length(rows_with_missing_before), "\n")
cat("Rows with missing values before deletion:\n")
print(NbValidS1[rows_with_missing_before, ])

```

```{r}
# Load the 'dplyr' library
library(dplyr)

# Remove rows containing specific values across the entire dataset
NbValidS1_cleaned <- NbValidS1 %>%
  filter_all(all_vars(!(. %in% c("inconnu", "?", "NA", "ND", "0", "", "NOT DEFINED"))))

# Display the new dataset without specific values
print(NbValidS1_cleaned)

# Remove rows with NA values in the 'NB_VALD' column
NbValidS1_cleaned <- NbValidS1_cleaned[!is.na(NbValidS1_cleaned$NB_VALD), ]


```

```{r}
#display the dimension of the new datset
dim(NbValidS1_cleaned)
```

Discussion:

Initially, the dataset **"NbValidS1"**dimension is 934851. We detected that there are rows 121338 containing missing values. We removed them and rechecked the data dimension, which is now 813513.

### **4. Data analysis and Visualization**

A. Temporal Trends : Distribution of Validations per Day (Jour):

```{r}
library(dplyr)
library(ggplot2)

# Extract a subset of NbValidS1_cleaned for the row with the maximum value, including the columns 'JOUR', 'NB_VALD', and 'LIBELLE_ARRET'
result <- NbValidS1_cleaned[which.max(NbValidS1_cleaned$NB_VALD), c("JOUR", "NB_VALD")]

# Display the result
print(result)

# Create a plot of the number of validations per day
plot_result <- ggplot(result, aes(x = JOUR, y = NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Day", y = "Number of Validations", title = "Day with Maximum number of validations S1 2018")

# Show the graph
print(plot_result)

# Save the graph as an image file (e.g., ".png", ".jpg")
ggsave("Day with Maximum number of validations S1 2019.png", plot_result, width = 10, height = 6, units = "in")



```

B. Spatial distribution : Distribution of Validations per Bus Stop and type of ticket (LIBELLE_ARRET et CATEGORIE_TITRE):

```{r}
# Extract a subset of NbValidS1_cleaned for the row with the maximum value,including the columns 'JOUR', 'NB_VALD', and 'LIBELLE_ARRET'
result <- NbValidS1_cleaned[which.max(NbValidS1_cleaned$NB_VALD), c( "JOUR","LIBELLE_ARRET", "NB_VALD","CATEGORIE_TITRE")]
#display the resultat
print(result)










```

```{r}
# Replace "NbValidS1_cleaned" with the actual name of your dataset
lowiest_vald_S1 <- NbValidS1_cleaned %>%
  arrange(NB_VALD) %>%
  slice_head(n = 2)

# Display the two rows representing the categories with the lowiest validation values
print(lowiest_vald_S1)

# Save the two rows to a CSV file
write.csv(lowiest_vald_S1, "lowiest_vald_S1.cvs", row.names = FALSE)


```

```{r}
# Convert the column NB_VALD to a numeric type
NbValidS1_cleaned$NB_VALD <- as.numeric(as.character(NbValidS1_cleaned$NB_VALD))

# Create a graph showing the distribution of ticket categories based on the number of validations
Tick_typ_valids_dist_S1 <- ggplot(data = NbValidS1_cleaned, aes(x = CATEGORIE_TITRE, y = NB_VALD)) +
  geom_bar(stat = "summary", fun = "sum", aes(fill = CATEGORIE_TITRE)) +
  labs(x = "Ticket Category", y = "Total Validations", title = "Distribution of Ticket Types by Validations") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Save the graph as an image file (e.g., ".png", ".jpg")
ggsave("Tick_typ_valids_dist_S1.png", Tick_typ_valids_dist_S1, width = 10, height = 6, units = "in")

# Display the graph
print(Tick_typ_valids_dist_S1)
```

### Conclusion:

-   During the first semester of 2019, starting in January and ending in June, our graphical analyses revealed that "NAVIGO" had the highest number of validations among all transportation passes 127535.

-   We can conclude that during this period, "NAVIGO" was the best-selling ticket. Additionally, the stations recording the highest number of validations were "La Défense - Grande Arche". It can be inferred that users likely have a profile of professionals, given that "La Défense"is the heart of Paris's economic activity.

-   Following Navigo, the transportation passes "IMAGINE R" and "TST"were also notable.Also We found that the stations "PORTE MAILLOT" "ALESIA" have the fewest validations, specifically fewer than 5 With transport ticket title is "Navigo Jour" in 01/01/2019.

-   The"14/02/2018 " is the day that had the highest number of validations, totaling 127535

-   Comparing 2018 and 2019, we see that the number of validations for transport tickets of type NAVIGO increased from 126411 to 127535.

    =\> So, the progress rate or percentage increase between 2018 and 2019 for the number of validations of NAVIGO transport tickets is approximately 0.88%

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

### B. Data cleaning, analysis , Visualization

We will proceed with the other datasets in the same way, following the same steps, to thoroughly analyze the number of validations in S2

#### **B. "NbValidS2" Data set:**

```{r}
dim(NbValidS2)
```

```{r}
# Search for missing values in all columns
rows_with_missing_before <- which(rowSums(is.na(NbValidS2) | NbValidS2 == "inconnu" | NbValidS2 == "?" | NbValidS2 == "NA" | NbValidS2 == "NON DEFINI" | NbValidS2 == "ND" | NbValidS2 == "0" | NbValidS2 == "") > 0)

# Display the number of rows with missing values before deletion
cat("Number of rows with missing values before deletion:", length(rows_with_missing_before), "\n")

# Display the rows with missing values before deletion
cat("Rows with missing values before deletion:\n")
print(NbValidS2[rows_with_missing_before, ])


# Load the 'dplyr' library
library(dplyr)

# Remove rows containing specific values across the entire dataset
NbValidS2_cleaned <- NbValidS2 %>%
  filter_all(all_vars(!(. %in% c("inconnu", "?", "NA", "ND", "0", "", "NOT DEFINED"))))

# Display the new dataset without specific values
print(NbValidS2_cleaned)

# Remove rows with NA values in the 'NB_VALD' column
NbValidS2_cleaned <- NbValidS2_cleaned[!is.na(NbValidS2_cleaned$NB_VALD), ]

dim(NbValidS2)
dim(NbValidS2_cleaned)
```

Discussion:

Initially, the dataset **"NbValidS2"**dimension is 953454 . We detected that there are 189343 rows containing missing values. We removed them and rechecked the data dimension, which is now 838833

#### 1. Temporal Trends : Distribution of Validations per Day (Jour):

```{r}
library(dplyr)
library(ggplot2)
NbValidS2 <- read.delim("/cloud/project/Data2018/NbValidS2.txt")

# Extract a subset of NbValidS1_cleaned for the row with the maximum value, including the columns 'JOUR', 'NB_VALD', and 'LIBELLE_ARRET'
result <- NbValidS2_cleaned[which.max(NbValidS2_cleaned$NB_VALD), c("JOUR", "NB_VALD")]

# Display the result
print(result)

# Create a plot of the number of validations per day
plot_result <- ggplot(result, aes(x = JOUR, y = NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Day", y = "Number of Validations", title = "Number of Validations per Day with Maximum")

# Show the graph
print(plot_result)



```

#### 2. Spatial distribution : Distribution of Validations per Bus Stop and type of ticket (LIBELLE_ARRET et CATEGORIE_TITRE:

```{r}
# Extract a subset of NbValidS1_cleaned for the row with the maximum value,including the columns 'JOUR', 'NB_VALD', and 'LIBELLE_ARRET'
result <- NbValidS2_cleaned[which.max(NbValidS2_cleaned$NB_VALD), c( "JOUR","LIBELLE_ARRET", "NB_VALD","CATEGORIE_TITRE")]
#display the resultat
print(result)

```

```{r}
# Replace "NbValidS1_cleaned" with the actual name of your dataset
lowiest_vald_S2 <- NbValidS2_cleaned %>%
  arrange(NB_VALD) %>%
  slice_head(n = 2)

# Display the two rows representing the categories with the lowiest validation values
print(lowiest_vald_S2)

# Save the two rows to a CSV file
write.csv(lowiest_vald_S2, "lowiest_vald_S2_data.cvs", row.names = FALSE)
```

```{r}
# Convert the column NB_VALD to a numeric type
NbValidS2_cleaned$NB_VALD <- as.numeric(as.character(NbValidS2_cleaned$NB_VALD))

# Create a graph showing the distribution of ticket categories based on the number of validations
Tick_typ_valids_dist_S2<- ggplot(data = NbValidS2_cleaned, aes(x = CATEGORIE_TITRE, y = NB_VALD)) +
  geom_bar(stat = "summary", fun = "sum", aes(fill = CATEGORIE_TITRE)) +
  labs(x = "Ticket Category", y = "Total Validations", title = "Distribution of Ticket Types by Validations") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Save the graph as an image file (e.g., ".png", ".jpg")
ggsave("Tick_typ_valids_dist_S2.png", Tick_typ_valids_dist_S2, width = 10, height = 6, units = "in")

# Display the graph
print(Tick_typ_valids_dist_S2)
```

#### 3. Spatiotemporal Analysis of Transport Ticket Validations during the Second Semester of 2017

-   During the second semester of 2019, starting in July and ending in december, our graphical analyses revealed that "NAVIGO" had the highest number of validations among all transportation passes 126202.

-   We can conclude that during this period, "NAVIGO" was the best-selling ticket. Additionally, the stations recording the highest number of validations were "La Défense - Grande Arche". It can be inferred that users likely have a profile of professionals, given that La Défense is the heart of Paris's economic activity.

-   Following Navigo, the transportation passes Imagine R and TST were also notable.Also We found that the stations "BOULOGNE-PONT DE SAINT CLOUD"\
    and "PORTE MAILLOT" and "ALESIA" have the fewest validations, specifically fewer than 5 With transport ticket title is "NON DEFINI" in 01/07/2019

-   The"08/10/2019 " is the day that had the highest number of validations, totaling 126202.

```{r}
# Load the dplyr library
library(dplyr)

# Convert the JOUR column to date format if it's not already in that format
NbValidS2_cleaned$JOUR <- as.Date(NbValidS2_cleaned$JOUR, format = "%d/%m/%Y")

# Specific dates
dates <- c("2019-07-01", "2019-07-14", "2019-12-31", "2019-12-25")

# Initialize a vector to store totals
totals <- c()

# Calculate the total number of validations for each specific date
for (date in dates) {
  filtered_data <- filter(NbValidS2_cleaned, JOUR == as.Date(date))
  total <- sum(filtered_data$NB_VALD)
  totals <- c(totals, total)
  cat("Number of validations on", date, ":", total, "\n")
}

# Find the date with the maximum validations
max_total <- max(totals)
date_with_max <- dates[which.max(totals)]

# Display the date with the maximum validations and its total
cat("\nDate with the highest number of validations:", date_with_max, "\n")
cat("Total validations for this date:", max_total, "\n")


```

#### 4. Comparison between the number of validations on a normal day, Christmas Day, the last day of the year, and National Day

The date 2019-07-14 had a important number of validations compraing with others event in ile de france : 2239917. Additionally, we observed a significant number of validations on 14/07/2019, indicating higher public movement due to the celebration of the French National Day."

Our goal is to analyse and discuss the number of validations on different specific dates, outlining the observations and the reasoning behind the variations in validation counts.

```{r}
# Load the dplyr and ggplot2 libraries
library(dplyr)
library(ggplot2)

# Convert the JOUR column to date format if it's not already in that format
NbValidS2_cleaned$JOUR <- as.Date(NbValidS2_cleaned$JOUR, format = "%d/%m/%Y")

# Specific dates
dates <- c("2019-07-01", "2019-07-14", "2019-12-25", "2019-12-31")

# Initialize a vector to store totals
totals <- c()

# Calculate the total number of validations for each specific date
for (date in dates) {
  filtered_data <- filter(NbValidS2_cleaned, JOUR == as.Date(date))
  total <- sum(filtered_data$NB_VALD)
  totals <- c(totals, total)
  cat("Number of validations on", date, ":", total, "\n")
}

# Create a data frame for plotting
results <- data.frame(Date = as.Date(dates, format = "%Y-%m-%d"), Validations = totals)

# Plotting the validation counts for specific dates with specified x-axis dates
plot <- ggplot(results, aes(x = Date, y = Validations)) +
  geom_bar(stat = "identity", fill = "skyblue", width = 0.5) +
  labs(title = "Validation Counts for Specific Dates in 2019",
       x = "Date",
       y = "Validation Counts") +
  scale_x_date(breaks = as.Date(dates, format = "%Y-%m-%d"), labels = dates) +  # Set breaks and labels
  theme_minimal()

# Show the plot
print(plot)

# Save the plot as an image file
ggsave("Validation Counts for Specific Dates in 2019.png", plot)




```

#### 5. Comparison of Ticket Validation Counts between Summer Vacation and Normal School Periods in 2017

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Convert the JOUR column to date format if it's not already in that format
NbValidS2_cleaned$JOUR <- as.Date(NbValidS2_cleaned$JOUR, format = "%d/%m/%Y")

# Define vacation and school periods
summer_vacation_start <- as.Date("2019-07-08")
summer_vacation_end <- as.Date("2019-09-03")
normal_period_start <- as.Date("2019-09-04")
normal_period_end <- as.Date("2019-10-10")

# Filter data for vacation and school periods
summer_data <- filter(NbValidS2_cleaned, JOUR >= summer_vacation_start & JOUR <= summer_vacation_end)
normal_data <- filter(NbValidS2_cleaned, JOUR >= normal_period_start & JOUR <= normal_period_end)

# Count the number of validations for each ticket type during vacation and school periods
summer_counts <- summer_data %>% 
  group_by(CATEGORIE_TITRE) %>% 
  summarise(Validation_Counts = sum(NB_VALD))

normal_counts <- normal_data %>% 
  group_by(CATEGORIE_TITRE) %>% 
  summarise(Validation_Counts = sum(NB_VALD))

print(summer_counts)
print(normal_counts)

# Merge data for plotting
combined_counts <- rbind(
  mutate(summer_counts, Period = "Summer Vacation"),
  mutate(normal_counts, Period = "Normal Period")
)

# Plot validation counts for ticket types during vacation and school periods
validation_plot <- ggplot(combined_counts, aes(x = CATEGORIE_TITRE, y = Validation_Counts, fill = Period)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(title = "Validation Counts for Ticket Types",
       x = "Ticket Type",
       y = "Validation Counts") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels for better readability
  scale_fill_manual(values = c("Summer Vacation" = "skyblue", "Normal Period" = "salmon")) +
  guides(fill = guide_legend(title = "Periods", nrow = 2))  # Add legend for fill colors

# Save the plot as an image file (e.g., ".png", ".jpg")
ggsave("validation_counts_plot_saison_s2.png", validation_plot, width = 10, height = 6, units = "in")

# Display the plot
print(validation_plot)

```

-   **Effect of Summer Vacation on IMAGINE R Transport Ticket Validations in Two Semesters of 2019**

To assess the impact of summer vacation on the validation count of IMAGINE R transport tickets, we compared two periods within the second semester of 2019.

The first period encompassed the summer school break, starting from ("2019-07-08") to ("2019-09-03"). During this time, we found that the validation count for NAVIGO transport tickets was 165404852, while for IMAGINE R tickets, it was 28036304.

The second period represents the regular timeframe from ("2019-07-04") to ("2019-10-10"). Here, the validation count for NAVIGO tickets was 135450920 and for IMAGINE R tickets, it reached 34018090.

We can also observe that the number of Navigo ticket validations during the summer is higher than during the normal period. This suggests that during the vacation period, there might be a higher number of tourists compared to the normal period.

=\> The rate of increase in IMAGINE R ticket validations between Period 1 and Period 2 is approximately 21.35%., we observe a significant impact from the summer school break on the validation

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

#### C. "ProfilFerS1" Data set:

```{r}
dim(ProfilFerS1)
```

```{r}
# Load data from file
ProfilFerS1 <- read.delim("/cloud/project/Data2019/ProfilFerS1.txt", header = FALSE)

dim(ProfilFerS1)
# Define column names
colnames(ProfilFerS1) <- c("CODE_STIF_TRNS", "CODE_STIF_RES", "CODE_STIF_ARRET", "LIBELLE_ARRET", "ID_REFA_LDA", "CAT_JOUR", "TRNC_HORR_60", "pourc_validations")

# Identify rows with specific or missing values
rows_to_remove <- which(rowSums(
  is.na(ProfilFerS1) | 
  ProfilFerS1 == "inconnu" | 
  ProfilFerS1 == "?" | 
  ProfilFerS1 == "NA" | 
  ProfilFerS1 == "NOT DEFINED" | 
  ProfilFerS1 == "ND" | 
  ProfilFerS1 == "0" | 
  ProfilFerS1 == ""
) > 0)

# Display the number of rows with specific or missing values before removal
cat("Number of rows with specific or missing values before removal:", length(rows_to_remove), "\n")

# Remove rows with specific or missing values
ProfilFerS1_cleaned <- ProfilFerS1[-rows_to_remove, ]

# Display the dimensions of the new dataset after cleaning
cat("Dimensions of the dataset after cleaning:", dim(ProfilFerS1_cleaned), "\n")



```

```         
```

Discussion:

Initially, the dataset **"ProfilFerS1"**dimension is 82998. We detected that there are 136 rows containing missing values. We removed them and rechecked the data dimension, which is now 82862.

#### 6. Determining the High Validation Percentage with Day Category, Locations, and Time Slot:

```{r}
# Commenting the code for English readability
library(dplyr)

# Convert the 'pourc_validations' column to numeric if necessary
ProfilFerS1_cleaned$pourc_validations <- as.numeric(as.character(ProfilFerS1_cleaned$pourc_validations))

# Find the row with the maximum percentage of validations
max_pourc_validations_row <- ProfilFerS1_cleaned %>%
  filter(pourc_validations == max(pourc_validations, na.rm = TRUE))

# Display the row with the maximum percentage of validations
print(max_pourc_validations_row)


```

Discussion:

While analyzing the ProfilFerS1 dataset to determine the type of day with the most validations, we discovered that 'SAVS' corresponds to a Saturday during school holidays, with a validation percentage of 100%. The location label is 'Dourdan la foret,' and the time slot is from 4 PM to 5 PM.

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

#### D. "ProfilFerS2" Data set:

```{r}
# Search for missing values in all columns
rows_with_missing_before <- which(rowSums(is.na(ProfilFerS2) | ProfilFerS2 == "inconnu" | ProfilFerS2 == "?" |ProfilFerS2 == "NA" | ProfilFerS2 == "NON DEFINI" |ProfilFerS2== "ND" | ProfilFerS2== "0" | ProfilFerS2== "") > 0)

# Display the number of rows with missing values before deletion
cat("Number of rows with missing values before deletion:", length(rows_with_missing_before), "\n")

# Display the rows with missing values before deletion
cat("Rows with missing values before deletion:\n")
print(ProfilFerS2[rows_with_missing_before, ])


# Load the 'dplyr' library
library(dplyr)

# Remove rows containing specific values across the entire dataset
NbValidS2_cleaned <- ProfilFerS2 %>%
  filter_all(all_vars(!(. %in% c("inconnu", "?", "NA", "ND", "0", "", "NOT DEFINED"))))

# Display the new dataset without specific values
print(ProfilFerS2_cleaned)

# Remove rows with NA values in the 'NB_VALD' column
NbValidS2_cleaned <- ProfilFerS2_cleaned[!is.na(ProfilFerS2_cleaned$NB_VALD), ]

dim(ProfilFerS2)
dim(ProfilFerS2_cleaned)

```

Discussion:

Initially, the dataset **"ProfilFerS2"**dimension is 82575. We detected that there are 138 rows containing missing values. We removed them and rechecked the data dimension, which is now 82860.

```{r}
# Commenting the code for English readability
library(dplyr)

# Convert the 'pourc_validations' column to numeric if necessary
ProfilFerS2_cleaned$pourc_validations <- as.numeric(as.character(ProfilFerS2_cleaned$pourc_validations))

# Find the row with the maximum percentage of validations
max_pourc_validations_row_S2 <- ProfilFerS2_cleaned %>%
  filter(pourc_validations == max(pourc_validations, na.rm = TRUE))

# Display the row with the maximum percentage of validations
print(max_pourc_validations_row_S2)

```
